{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Hybrid Demucs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importing required libraries\n",
        "import torch\n",
        "import torchaudio\n",
        "\n",
        "# Printing the versions of torch and torchaudio\n",
        "print(torch.__version__)\n",
        "print(torchaudio.__version__)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Audio\n",
        "from mir_eval import separation\n",
        "from torchaudio.pipelines import HDEMUCS_HIGH_MUSDB_PLUS\n",
        "from torchaudio.utils import download_asset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bundling the HDEMUCS_HIGH_MUSDB_PLUS pipeline\n",
        "bundle = HDEMUCS_HIGH_MUSDB_PLUS\n",
        "\n",
        "# Getting the model from the bundle\n",
        "model = bundle.get_model()\n",
        "\n",
        "# Saving the model's state dictionary to a file\n",
        "torch.save(model.state_dict(), 'hdemucs_model.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bundling the HDEMUCS_HIGH_MUSDB_PLUS pipeline\n",
        "bundle = HDEMUCS_HIGH_MUSDB_PLUS\n",
        "\n",
        "# Getting the model from the bundle\n",
        "model = bundle.get_model()\n",
        "\n",
        "# Loading the trained model's state dictionary from a file\n",
        "model.load_state_dict(torch.load('hdemucs_model.pth', map_location='cpu'))\n",
        "\n",
        "# Transfer the model to CPU\n",
        "device = torch.device(\"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Get the sample rate from the bundle\n",
        "sample_rate = bundle.sample_rate\n",
        "\n",
        "# Printing the sample rate\n",
        "print(f\"Sample rate: {sample_rate}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchaudio.transforms import Fade\n",
        "\n",
        "def separate_sources(\n",
        "    model,\n",
        "    mix,\n",
        "    segment=10.0,\n",
        "    overlap=0.1,\n",
        "    device=None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Apply a model to a given mixture, using fading and adding segments together to process the model segment by segment.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The model to apply to the mixture.\n",
        "        mix (torch.Tensor): The input mixture tensor with shape (batch, channels, length).\n",
        "        segment (float): Segment length in seconds.\n",
        "        overlap (float): Overlap factor between segments (0 to 1).\n",
        "        device (torch.device, str, or None): Device on which to execute the computation.\n",
        "            If None, the device of the input mix tensor is assumed.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: The separated sources tensor with shape (batch, sources, channels, length).\n",
        "    \"\"\"\n",
        "    # Set the device for computation\n",
        "    if device is None:\n",
        "        device = mix.device\n",
        "    else:\n",
        "        device = torch.device(device)\n",
        "\n",
        "    # Get the sample rate from the model bundle\n",
        "    sample_rate = model.sample_rate\n",
        "\n",
        "    # Calculate chunk length and overlap frames\n",
        "    chunk_len = int(sample_rate * segment * (1 + overlap))\n",
        "    overlap_frames = overlap * sample_rate\n",
        "\n",
        "    # Initialize fade transform\n",
        "    fade = Fade(fade_in_len=0, fade_out_len=int(overlap_frames), fade_shape=\"linear\")\n",
        "\n",
        "    # Initialize output tensor\n",
        "    batch, channels, length = mix.shape\n",
        "    final = torch.zeros(batch, len(model.sources), channels, length, device=device)\n",
        "\n",
        "    # Process the mixture segment by segment\n",
        "    start = 0\n",
        "    end = chunk_len\n",
        "    while start < length - overlap_frames:\n",
        "        chunk = mix[:, :, start:end]\n",
        "        with torch.no_grad():\n",
        "            out = model.forward(chunk)\n",
        "        out = fade(out)\n",
        "        final[:, :, :, start:end] += out\n",
        "\n",
        "        # Update start and end indices for next segment\n",
        "        if start == 0:\n",
        "            fade.fade_in_len = int(overlap_frames)\n",
        "            start += int(chunk_len - overlap_frames)\n",
        "        else:\n",
        "            start += chunk_len\n",
        "        end += chunk_len\n",
        "\n",
        "        # Adjust fade-out length for the last segment\n",
        "        if end >= length:\n",
        "            fade.fade_out_len = 0\n",
        "\n",
        "    return final\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download the sample song if needed\n",
        "# SAMPLE_SONG = download_asset(\"tutorial-assets/hdemucs_mix.wav\")\n",
        "\n",
        "# Alternatively, use a local sample song file\n",
        "SAMPLE_SONG = \"sample-12s.wav\"\n",
        "\n",
        "# Load the waveform and sample rate from the sample song\n",
        "waveform, sample_rate = torchaudio.load(SAMPLE_SONG)\n",
        "waveform = waveform.to(device)\n",
        "mixture = waveform\n",
        "\n",
        "# Print waveform and sample rate information\n",
        "print(\"Waveform: \", waveform)\n",
        "print(\"Sample Rate: \", sample_rate)\n",
        "\n",
        "# Set parameters for separating tracks\n",
        "segment: int = 10\n",
        "overlap = 0.1\n",
        "\n",
        "print(\"Separating track\")\n",
        "\n",
        "# Perform normalization on the waveform\n",
        "ref = waveform.mean(0)\n",
        "waveform = (waveform - ref.mean()) / ref.std()\n",
        "\n",
        "# Separate sources using the `separate_sources` function\n",
        "sources = separate_sources(\n",
        "    model,\n",
        "    waveform[None],\n",
        "    device=device,\n",
        "    segment=segment,\n",
        "    overlap=overlap,\n",
        ")[0]\n",
        "\n",
        "# Denormalize the separated sources\n",
        "sources = sources * ref.std() + ref.mean()\n",
        "\n",
        "# Convert the separated sources to a list and create an audio dictionary\n",
        "sources_list = model.sources\n",
        "sources = list(sources)\n",
        "audios = dict(zip(sources_list, sources))\n",
        "\n",
        "# Print the list of separated sources\n",
        "print(sources_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define parameters for the Spectrogram transform\n",
        "N_FFT = 4096  # Number of FFT points\n",
        "N_HOP = 4     # Hop length\n",
        "\n",
        "# Create the Spectrogram transform\n",
        "stft = torchaudio.transforms.Spectrogram(\n",
        "    n_fft=N_FFT,\n",
        "    hop_length=N_HOP,\n",
        "    power=None,  # Compute power spectrogram\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display the full audio mixture using IPython's Audio widget\n",
        "Audio(mixture, rate=sample_rate)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drums Audio\n",
        "Audio(audios[\"drums\"], rate=sample_rate)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bass Audio\n",
        "Audio(audios[\"bass\"], rate=sample_rate)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Vocals Audio\n",
        "Audio(audios[\"vocals\"], rate=sample_rate)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Other Audio\n",
        "Audio(audios[\"other\"], rate=sample_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
